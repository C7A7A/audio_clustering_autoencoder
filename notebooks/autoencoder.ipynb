{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b789c2b4",
   "metadata": {},
   "source": [
    "<b>Autonecoder tutorial:</b>\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html\n",
    "</br>\n",
    "<b>Deep Clustering with Convolutional Autoencoders paper:</b>\n",
    "https://xifengguo.github.io/papers/ICONIP17-DCEC.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63cc7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e0a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch version:\u001b[39m\u001b[38;5;124m\"\u001b[39m,torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\audio\\Lib\\site-packages\\torch\\cuda\\__init__.py:419\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \n\u001b[0;32m    410\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_device_properties(device)\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\audio\\Lib\\site-packages\\torch\\cuda\\__init__.py:449\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     _lazy_init()  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    450\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\audio\\Lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\",torch.__version__)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e16062",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75067f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fd3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfbb8652",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0fdfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. try other activation_fns\n",
    "# 2. change comments to proper sizes\n",
    "# 3. change values to proper sizes\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "        - num_input_channels : Number of input channels of the image.\n",
    "        - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "        - latent_dim : Dimensionality of latent representation z\n",
    "        - activation_fn : Activation function used throughout the encoder network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_channels: int,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        activation_fn: object = nn.GELU\n",
    "    ): \n",
    "\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, base_channel_size, kernel_size=3, padding=1, stride=2), # 32x32 -> 16x16\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(base_channel_size, base_channel_size, kernel_size=3, padding=1),\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(base_channel_size, 2 * base_channel_size, kernel_size=3, padding=1, stride=2), # 16x16 -> 8x8\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(2 * base_channel_size, 2 * base_channel_size, kernel_size=3, padding=1),\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(2 * base_channel_size, 2 * base_channel_size, kernel_size=3, padding=1, stride=2), # 8x8 -> 4x4\n",
    "            activation_fn(),\n",
    "            nn.Flatten(), # flatten to single feature vector\n",
    "            nn.Linear(2 * base_channel_size * 4 * 4, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2928b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. try other activation_fns\n",
    "# 2. change comments to proper sizes\n",
    "# 3. change values to proper sizes\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "        - num_input_channels : Number of input channels of the image.\n",
    "        - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "        - latent_dim : Dimensionality of latent representation z\n",
    "        - activation_fn : Activation function used throughout the encoder network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_channels: int,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        activation_fn: object = nn.GELU\n",
    "    ): \n",
    "\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2 * base_channel_size * 4 * 4),\n",
    "            activation_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * base_channel_size, 2 * base_channel_size, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4 -> 8x8\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(2 * base_channel_size, 2 * base_channel_size, kernel_size=3, padding=1),\n",
    "            activation_fn(),\n",
    "            nn.ConvTranspose2d(base_channel_size, 2 * base_channel_size, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 -> 16x16\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(base_channel_size, base_channel_size, kernel_size=3, padding=1),\n",
    "            nn.ConvTranspose2d(base_channel_size, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 -> 32x32\n",
    "            nn.Tanh() # TODO: scale images between -1 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4) # reshape tensor (spatial dimensions set to 4x4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d26916c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. change values to proper sizes\n",
    "# 2. try different loss functions\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "        - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "        - latent_dim : Dimensionality of latent representation z\n",
    "        - encoder_class : Encoder\n",
    "        - decoder_class : Decoder\n",
    "        - num_input_channels : Number of input channels of the image.\n",
    "        - width : Image width.\n",
    "        - height : Image height.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        encoder_class: object = Encoder,\n",
    "        decoder_class: object = Decoder,\n",
    "        num_input_channels: int = 3,\n",
    "        width: int = 32,\n",
    "        height: int = 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        \n",
    "        # Example input array needed for visualizing the graph of network\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "        \n",
    "    \"\"\"\n",
    "    The forward function takes in an image and returns the reconstructed image\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a batch of images, this function returns the reconstruction loss (MSE in our case)\n",
    "    \"\"\"\n",
    "    def _get_recontruction_loss(self, batch):\n",
    "        x, _ = batch # Skip labels\n",
    "        x_hat = self.forward(x)\n",
    "        \n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters, lr=1e-3)\n",
    "        \n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=0.2,\n",
    "            patience=20,\n",
    "            min_lr=5e-5\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_schedule\": scheduler,\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_recontruction_loss(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_recontruction_loss(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_recontruction_loss(batch)\n",
    "        self.log('test_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5556d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
